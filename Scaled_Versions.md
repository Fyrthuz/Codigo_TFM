Below is a comparison table that summarizes the main components of the three implementations—with the first one updated to use Hamming distance statistics for uncertainty estimation:

| Component                      | Implementation 1 (Modified – Hamming Distance)                                                                                                                                                                                                                                                                             | Implementation 2 (Streamlined – Error–Entropy Covariance)                                                                                                                   | Implementation 3 (CalibratedMCDropout – Cross-Entropy Loss)                                                                                                                  |
|--------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **MC Sampling**                | Runs multiple forward passes (mc_samples) collecting both log probabilities and discrete predictions (via softmax then argmax) to later compute pairwise Hamming distances.                                                                                                                                         | Runs multiple forward passes to collect log probabilities (via softmax) for averaging.                                                                                         | Runs multiple forward passes to collect softmax probabilities.                                                                                                              |
| **Loss Computation**           | Averages log probabilities across MC samples and computes the NLL loss on the averaged predictions.                                                                                                                                                                                                                      | Averages log probabilities and computes NLL loss on the averaged output.                                                                                                      | Computes categorical cross-entropy loss on the averaged probabilities (using a log conversion with an EPS safeguard).                                                        |
| **Uncertainty Computation**    | Computes pairwise Hamming distances between discrete MC predictions; calculates their expectation (exp_hd) and variance (var_hd) and defines the base uncertainty scale as: <br>  scale = exp_hd × (1 + var_hd)                                                                                                         | Computes prediction entropy from averaged probabilities and then derives the base scale as the inverse of the covariance between entropy and a binary error indicator.      | Computes prediction entropy from averaged probabilities and derives the base scale as the inverse of the covariance between entropy and prediction error (binary indicator). |
| **Calibration Method**         | Uses an iterative Expected Calibration Error (ECE) loop (_find_relaxed_scale) to adjust the base scale and obtain a “relaxed” scale for better calibration.                                                                                                                                                         | Uses an iterative ECE-based calibration loop to refine the base scale into a relaxed (calibrated) scale.                                                                        | Uses an iterative ECE-based loop to fine-tune the uncertainty scale, yielding a relaxed calibrated scale.                                                                   |
| **Dropout Mechanism**          | Uses a custom MCDropout wrapper to set and enable dropout (p) in the model’s dropout layers.                                                                                                                                                                                                                              | Uses the same MCDropout wrapper to set dropout probability p across the model.                                                                                                | Uses the same MCDropout mechanism to adjust dropout probability across model layers.                                                                                        |
| **Return Values**              | Returns the optimal dropout probability (phi), the base uncertainty scale (from Hamming stats), and the relaxed (calibrated) scale.                                                                                                                                                                                       | Returns the optimal dropout probability (phi), the base uncertainty scale (from error–entropy covariance), and the relaxed scale after calibration.                         | Returns the optimal dropout probability (phi), the base uncertainty scale, and the relaxed (calibrated) scale.                                                                |
| **Specific Notes**             | Leverages disagreement among MC predictions (via pairwise Hamming distances) to capture uncertainty diversity. This approach may be computationally more intensive but provides extra insight into prediction variability.                                                                                          | Focuses solely on the relationship between prediction error and entropy, resulting in a more streamlined and computationally efficient approach.                            | Similar in design to Implementation 2, but emphasizes the use of categorical cross-entropy for segmentation loss and includes explicit handling for segmentation tensor shapes. |

This table highlights that while all three approaches use MC dropout and iterative ECE-based calibration, the modified first implementation uniquely incorporates Hamming distance statistics to inform the uncertainty scale, whereas the second and third rely on the error–entropy covariance—with the third version using cross-entropy loss specifically for segmentation tasks.